# network architecture
# encoder related
encoder: transformer
encoder_conf:
    output_size: 256    # dimension of attention
    attention_heads: 4
    linear_units: 2048  # the number of units of position-wise feed forward
    num_blocks: 12      # the number of encoder blocks
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    attention_dropout_rate: 0.0
    input_layer: conv2d # encoder architecture type
# decoder related
decoder: transformer
decoder_conf:
    attention_heads: 4
    linear_units: 2048
    num_blocks: 6
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    self_attention_dropout_rate: 0.0
    src_attention_dropout_rate: 0.0

# hybrid CTC/attention
e2e_conf:
    ctc_weight: 0.3
    lsm_weight: 0.1     # label smoothing option

# minibatch related
batch_size: 32

# optimization related
optim: adam
optim_conf:
    lr: 10.0
accum_grad: 2
grad-clip: 5
patience: 0
max_epoch: 20
val_scheduler_criterion:
- eval
- loss
scheduler: noamlr
