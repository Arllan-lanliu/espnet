# minibatch related
batch-size: 20
maxlen-in: 800
maxlen-out: 150

# optimization related
sortagrad: 0
opt: adadelta
epochs: 20
patience: 3

# network architecture
## encoder related
etype: vggblstmp
elayers: 4
eunits: 320
eprojs: 320
## decoder related
dlayers: 1
dunits: 300
## attention related
atype: location
adim: 320
aconv-chans: 10
aconv-filts: 100
## joint network related
joint-dim: 320

# rnn-t related (0:rnnt, 1:rnnt-att)
rnnt-mode: 1

# finetuning related
## Note : Current implementation only allow to do pre-initialization with models
##        matching the configuration specified above. The architecture you specify
##        should match the modules architecture you want to do finetuning with.
##        For example, if you want to pre-initialize the decoder embedding layer
##        from a specified model, "dec-embed-dim" param should be set accordingly.

# following model correspond to conf/tuning/train_mtlalpha1.0.yaml
#enc-init: "exp/tr_it_pytorch_train_mtlalpha1.0/results/model.loss.best"

# following model correspond to conf/tuning/train_mtlalpha0.0.yaml
#dec-init: "exp/tr_it_pytorch_train_mtlalpha0.0/results/model.loss.best"